Application Architecture:
    - Which Architecture should we follow?
    => it depend on bussiness requirment
    - Architectures:   
        - Monolithic Architecture: Monolithic architecture refer to a software application design all components are tightly coupled and integrate into a single, self -contained unit.(single tier architecture)
                                   - key characteristics
                                      - single codebase
                                      - centralized deployment
                                      - shared database
                                      - tight coupling
                                   - advantage
                                   - disadvantage
                                   - suitable for:
                                      - simple application: where the functionality is limited and unlikely to changes significatly
                                      - small team: where resource are limited
                                      - POC: where the focus is on validating a concept before investing in a complex architecture

            - 2-Tier Architecture:in this software architecture pattern that divides the application into two layers
                 - 1. presentation tier: this layer handle the user interface and intracts with the user directly
                 - 2. data tier (server): this layer handle bussiness logic and data store and processing
                 - advantage: improve scalablity, enhance mentainablity, increse security, flexible thechnical choose, simplify deployment.
                 - disadvantage: increse complex, additional network trafic, signle point failour(if server fails the entier application unavaliable)
                 - suitable for:
            - N Tier Architecture: also known as multi tier architecture is a software architecture pattern that separate the application into multiple layer.
                        - advantage: improve scalablity, enhance mentainablity, increse security, simplify deployment, thechnical choose
                        - disadvantage: increse security, additional network trafic, increse debugging defficulty
                        - suitable for: large web application with complex bussiness logic, enterprize application that require.
            - Modular Monolithic Architecture: a Modular Monolithic architecture is a software development approch..
                       - single codebase
                       - independent modules
                       - loose coupling
                       - shared resources
                       - advantage
                       - disadvantage
                       - suitable for  
            - Microservice Architecture: Microservice architecture is a software development approch that structure an application as a collection of small, independent, and loosely coupled service. each service is self-contained and focuses on single bussiness capability.services communicate well defined apis
                  - key characteristics
                    - independent service
                    - loose coupling
                    - lightweight technology
                    - decentralize data
                - advantage: 
                    - improve scalablity
                    - enhance agility
                     - increse fauilt tolarance
                     - simplify mentainablity
                     - flexible technology choose
                - disadvantage: 
                    - increse complex
                    - distribute debugging
                    - infrastructure overhead
                    - network latency
                    - data consistency
                - suitable for: large and complex application that require scalablity, application with evolving requirment and frequent updates. 
            - Event-Driven Architecture: is a software design pattern that focuses on events significate occurance or changes within a system - to drive application logic and communication. instead of traditional request, response intractions, events trigger actions and updates across diffrent services.
               - main key:
                  - events
                  - events producer
                  - events comsumer
                  - events broker
                - advantage:
                   - scalablity
                   - real time processing
                   - async communicate
                - disadvantage: 
                    - increse complex
                    - monitoring and tracing
                    - data consistency
                    - debugging difficult
                - suitable for
            - Cloud Native Architecture: this is refers to methology for designing and building, software application, specifically for development and operation in the cloud computing envionment
                - key
                   - Microservice
                   - containers
                   - continous delivery
                - advantage
                - disadvantage
                - suitable for
            - Serverless Architecture: is a cloud computing execution model where the cloud provider
              - key
                - no server management
                - pay-per use
                - scalablity
                - event drivent
                - Microservice
                - stateless
              - advantage
              - disadvantage
              - suitable for   
            - How to choose the perfect Architecture?
              => there is no defination for perfect architecture, start with minimum effort and budget  
            - What if I want to make a wise decision? 
               - Focus on the following factors:
                  - project requirment
                     - complexity
                     - scalablity
                     - performance
                     - mentainablity
                     - security
                     - change frequent
                  - team expertices
                     - team experiance
                     - require skill set
                  - timeline
                     - respected deadline
                  - budget
                     - development
                     - deployment







improve the application performance:

What is performance?
=> application performance refers to the 'how well' your software application operations in terms of various metrics. it encompasses servral key aspects, including:
   - responsiveness
   - stability
   - scalablity
   - resource utilization
   - user experiance

for a backend application
   - measure of how fast or responsive  a system in under
      - a given workload
          - backend data
          - request valume
      - a given hardware
          - kind
          - capacity

how to spot a performance problem?
=> spotting a performance problem require vigilance and undersatand of your system normal behavior.
    - monitoring metrics
       - incresed response time: pages taking longer to load, delays in progressing inputs, or unresponsive application are telltale signs.
       - higher error rate: frequent crashes, failed transactions, or application error indicate underlying performance issues.
       - resource utilization: spikes in cpu usage, memory exhaustion or high network brandwidth consumption can point to bottlenecks.
       - log analysis: reviewing system logs may reveal specific error warnings or performance related events.
    - user feedback
       - complain about slowness: users reporting sluggish performance, unresponsive application.
       - incresed support tickets: A rise in supports tickets related to performance problem further reinforces the need for invesigatiom
    - proactive checks
       - load testing
       - performance profiling
       - benchmarking
    - visual indicator
       - slow loading animations or progress bars
       - log in user interface

every performance problem is the result of some queue building somewhere

where does a queue can build up ?
 - network:
    - incresed latency: ping times become higher, uploads and download take longer, and timeouts occur more frequently.
    - packet loss: data packets further get dropped, leading to incomplete information and retransmission requests, further extending wait times.
    - full send/recive buffer: Queues on the network card or operating system fill up, preventing data from being sent or received efficiently.
 - database:
    - slow query execution:  Long wait times for database queries to complete, impacting page loading times and user actions.
    - database connection pool execution: All available database connections are in use, preventing new requests from being served, leading to user wait times.
    - disk i/o bottleneck: Queues for reading and writing data to the disk build up, impacting database operations and application responsiveness.
 - operating system
    - High cpu utilization
    - excessive memory usage
    - long progress wait times
 - application code
    - ineffcient algorithms
    - redundant calculation
    - code deadlocks

queue build up root cause
  - badly written code
    - ineffcient algorithms, unnecessary calculation, or poor memory management can all contribute to slow processing.
  - external dependencies
   - relying on slow external service or resource intensive libraries can also add to the processing time.
  - database access
    - multiple requests needing to connect to a database will form a queue waiting for available connections.
  - serial execution in code
    - if tasks are coded to run sequentially one after the other, any bottleneck in the chain will slow down the entier processs

Limited resource capacity: This is essentially a physical limitation of the system. For example, having only one CPU means that only one task can be actively processed at a time. Any incoming requests beyond that will have to wait for their turn, forming a queue.
Other limited resources can also create bottlenecks, such
Memory: Insufficient memory can force the system to

swap data to disk, significantly slowing down processing. Network bandwidth: Limited network bandwidth can create queues for data transfer, impacting download and upload speeds.

By adopting this queue-based perspective, we can effectively spot performance issues early on, diagnose their root causes, arid implement targeted solutions to keep our system running smoothly.

Inefficient slow processing: This refers to a situation where the code responsible for handling tasks takes too long to execute. This can happen due to various factors, like:
Badly written code: Inefficient algorithms, unnecessary calculations, or poor memory management can all contribute to slow processing.
Serial resources access: This happens when accessing & resource requires tasks to be executed one after another, creating a queue as requests wait for their turn. Examples include:
Database access: Multiple requests. needing to connect to a database will form
a queue waiting for available connections Shared files: If only one thread can access a file at a time, others will have to wait in line.
External dependencies: Relying on slow external services or resource-intensive libraries can also add to the processing time.
Serial execution in code: If tasks are coded to run sequentially, one after the other, any bottleneck in the chain will slow down the entire process.

Note
We should always try to avoid building queues when designing a new system or find where the is queue building in an existing system.

performance principales:
   - efficiency: reduce response time on a single request
       - Efficient Resource Utilization (System resources utilization)

          - a 10-Memory, Network, Disk
          - b CPU
       - Efficient Logic
          - a Algorithms
          - b DB Queries
       - Efficient Data Storage
          - a Data Structures
          - b DB Schema
       - Caching
   - concurrency: reduce response time on a concurrent request
       - Hardware (must have concurrency support
           - multiple core)
           - Software (we need to code our software that can utilize multiple core)
              - a Queuing (this queue is not going to block requests)
              - b. Coherence (being logical)
   - capacity: increse hardware improve performance
               - CPU
               - Memory
               - Disk
               - Network
   
   performance measurement metrics
     - latency
       - Measured in time : Focuses on how quickly the system responds to a request.
       - Affects user experience: Focuses on how quickly the system responds to a request.
       - Desired State: low as possible to keep users satisfied and maintain smooth interaction.
     - throughput
        - Measured in Request Per Unit of Time: Indicates the system's ability to handle concurrent requests.
        - Affects number of users supported: Focuses on how quickly the system responds to a request.
        - Desired State: Greater than the request rate to avoid backlogs and maintain responsiveness under peak load.
     - errors
       - Percentage of requests resulting in errors: Represents the system's reliability and correctness.
       - Affects functional correctness:Errors can disrupt functionality and lead to user frustration.
       - Desired State: None, aiming for zero errors to ensure accurate and reliable operation.
     - resource saturation
       - Measured in Percentage: Indicates how busy system resources (CPU, memory, network) are at any given time,
       - Affects hardware capacity required: Over-saturation can lead to performance bottlenecks and potential hardware upgrades.
       - Desired State: Efficient utilization of all resources, avoiding both underutilization (waste) and overutilization (bottlenecks)


 - Tail Latency
   Tail latency refers to the response times of the slowest requests within a certain set, typically measured at the 99th or 99.9th percentile. It provides a more comprehensive picture of performance compared to focusing solely on average latency, which can be misleading if there are a few outliers with significantly higher response times.
      - Tail latency is an indication of queuing of requests. Gets worse with higher workloads
      - Average latency hides the effects of tail latency.
   - Additional Considerations: 
      - It's crucial to define realistic target values for each metric based on your specific system, user base, and workload characteristics.
      - Monitor these metrics continuously to identify trends, bottlenecks, and potential issues before they impact users.
      - Optimize your system based on these metrics, focusing on reducing latency, improving throughput, minimizing errors, and optimizing resource utilization.
      - Remember, these metrics often interact and influence each other. Optimizing one might affect others, requiring a holistic approach to performance optimization.
  - Serial Request Latency
    - Network Connection: There are two types of connections -
        - Internet (Public)
        - Intranet (Private)
        - Intranet is a much reliable connection and faster
    - TCP Handshake
        - Client   <----> Server
        - State changes to SYN-SENT
        - SYN seq: 100
        - State changes to ESTABLISHED
        - SYN-ACK
        - seq: 200
        - ack:
        - ACK
        - seq: 101
        - ack
        - State changes to
        - State changes to SYN-RECEVED
    - TLS Handshake
        - Client
        - Server
        - Establish TCP Connection
        - SYN
        - SYN ACK
        - Client Hello
        - SSL/TLS Handshake
        - Server Hello Certificate
        - Certificate Vers
        - Server Finished
        - Encrypted Application Data
        - HTTP GET
        - HTTP Response
    - Network Latency
      - Data Transfer Latency - Every computer is connected via wire. It takes some time travel data from point A to point B. That is called data transfer latency.
      - TCP Connection - Creation or Establish a connection also has some latency
      - SSL/TLS Connection - Much more overhead
    - Network Latency - Approaches
  - TCP Connections:
    - Connection Pool (DB App Server): Reusing existing connections instead of establishing new ones for every request can significantly reduce connection setup time and latency.
    - Persistent Connections (Client - API Gateway):
    - HTTP/1.1 automatically maintains persistent connections, eliminating the need for handshakes in subsequent requests, improving latency.
    - SSL Session Caching: Reusing established SSL sessions minimizes the overhead of handshakes, especially for HTTPS connections, saving bandwidth and reducing latency.
  - Data Transfer.
    - Session / Data Caching: Caching frequently accessed data at the client or intermediate servers (eg. API Gateway) avoids fetching it from the origin server, reducing round-trip time and latency.
    - Static Data Caching: Cache static content like logos, scripts, and images close to the user (eg, CDN) to minimize fetching time and improve page load speed. Data Format & Compression: Choosing efficient data formats (eg. JSON over XML) and compression techniques can significantly reduce data size, leading to faster transmission and lower latency.
    - Binary Data-gRPC: Using protocols like gRPC optimized for binary data transfer avoids unnecessary text parsing and serialization/deserialization, further reducing latency for transferring large data sets.

  - Network Latency - Approaches
    -Additional considerations:
       - Minimize HTTP requests: Combine resources into fewer requests to reduce round-trip times and network overhead.
       - Optimize DNS resolution: Use a Content Delivery Network (CDN) with strategically placed caching servers to minimize DNS lookup times..
       - Monitor and analyze: Continuously monitor network performance metrics like latency, throughput, and packet loss to identify bottlenecks and areas for further optimization.

   - Memory Access Latency
      - Finite Heap Memory (If we reach the limit the app will crash)
      - Large Heap Memory (The application may occupy a large number of memory)
      - GC (garbage collector) Algorithm.
      - Finite Buffer Memory (on DB)
   - Minimize Memory Access Latency
      - 1. Avoid Memory Bloat: This is the crucial principle. Reduce the overall amount of data your application holds in memory at any given time.
      - 2. Weak/Soft References: Utilize these reference types to indicate data that can be reclaimed by the garbage collector if resources become scarce. This helps prevent unnecessary data being held in memory permanently.
      - 3. Multiple Smaller Processes: Splitting your application into smaller, focused processes can help reduce individual memory footprints.
      - 4. Garbage Collection Algorithm: Choosing an efficient GC algorithm and tuning its parameters can significantly impact latency. Opt for algorithms with low pauses and efficient memory reclamation to minimize overhead during GC cycles.
      - 5. Batch Processes: Process data in batches instead of single items. This can improve memory efficiency by reducing context switching and increasing data locality within cache lines.
      - 6. Constantly run with main process: if possible, avoid creating and destroying the main process frequently. This minimizes memory allocation and deallocation overhead, reducing latency associated with these actions.
      - 7. Database Normalization: Implement proper database normalization to avoid redundant data storage. This reduces the memary footprint on the database side and minimizes unnecessary data transfers to the application, improving overall access times.
      - 8. Computer Over Storage: Prioritize processing data in memory where possible instead of relying on disk storage. This significantly reduces access latency compared to fetching data from slower disks.
      - 9. Don't store unnecessary data: Carefully analyze your data needs and avoid storing information that can be easily calculated on demand. This frees up valuable memory space and access to truly essential data

   -Disk Access Latency
     -Disk IO is very slow.
        -Web Content Files
        -Logging
        -DB Disk Access (Very Very Important)

   -Minimize Memory Access Latency
        -1. Sequential & Batch IO: Disks are significantly faster at writing and reading data sequentially compared to random access. Batching operations together and writing sequentially can dramatically improve performance.
        -2. Asynchronous Logging (Worker Thread): Offload logging to a separate worker thread to avoid blocking the main thread and slowing down critical operations.
        -3. Web Content Caching: Cache static content like images, scripts, and stylesheets closer to the user (e.g., CDN) to minimize disk access on the origin server.
        -4. Reverse Proxy / Load Balancer: Utilize these tools to distribute traffic across multiple servers, reducing the burden on individual disks and preventing bottlenecks. Additionally, reverse proxies can cache content further improving access times.
        -5. Separate hosting of dynamic and static data: Host dynamic content, that frequently changes, on separate servers or storage systems from static content.
        -6. Page Cache, Zero Copy: Utilize the operating system's page cache to keep frequently accessed data readily available in memory, minimizing disk access for repeated requests.
        -7. Query Optimization: Efficiently written database queries minimize disk access by retrieving only the necessary data.
        -8. Data Caching: Cache frequently accessed database results within the application to further reduce disk access for repeated requests.
        -9. Schema Optimization: Design your database schema with efficient data layout and appropriate indexing to minimize disk seeks and improve query execution speed.
        -10. Higher IOPS, RAID, SSD Disk: Consider hardware upgrades like disks with higher IOPS (Input/Output Operations per Second) or RAID configurations for improved data access parallelism. SSDs offer significantly faster access times compared to tradit HDDs.


   -CPU Latency
      -Inefficient Algorithms (Developer can handle)
      -Context Switching
        - In computing, a context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point, and then restoring a different, previously saved, state.
CPU
↓
Interrupt or system call
↓
Save state into PCB1
↓
Reload state from PCB2
↓
Interrupt or system call
↓
Save state into PCB2
↓
Reload state from PCB1


Process P1
↓
Executing
↓
↓
Idle
Executing


Process P2
↓
Idle
↓
Executing
↓
Idle

  - Minimize CPU Latency
     - Efficient Algorithms
     - Efficient Queries
     - Batch/Async IO
     - Single Threaded Model
     - Thread Pool Size
     - Multi-Process in Virtual Environment

    - Latency Costs
    - Concurrent Request: It requires knowledge of Operating System and Multithreading
    - capacity: increse hardware improve performance
    - caching: we will have a separate sessions for that










Scalablity of Application

we optimization the application performance to make our client happy

current system performance
    - req-res latency 150 ms per request average, 350 ms tail latency, throughput - 1000 concurrect request per Second

whole day the application performance really well, howerver during the peak hour the system gets shut down.

what we can do next ?
we need to scale our system

scaling in system design
  - Scaling in system design refers to the ability of a system to handle increased load or demand while maintaining or improving its performance. Scaling is an essential aspect of system design, especially in the context of distributed and cloud-based applications, where workloads can vary over time and may experience rapid growth. The goal of scaling is to ensure that a system can adapt to changing circumstances, such as increased user traffic or growing datasets, without compromising its responsiveness or reliability.

Performance vs Scaling

Performance:
  • Low Latency
  • High Throughput
      • Concurrency
           • Single Machine (Multi Threading)
           • Multi Machine (Multi Threading + Multi Processing = Distributed Processing)
      • Capacity
   • Capacity

Scalability
   • Subset of Performance
   • High Throughput
       • Ability of a system to increase its throughput by adding more hardware capacity
   • Scale both up and down

There are two types of scaling
   - vertical
   - horizontal

  Vertical Scaling - Vertical scaling, also known as scaling up or scaling vertically, refers to the process of increasing the capacity of a single server or resource in order to handle a larger load or improve performance. This is typically achieved by adding more CPU, memory, storage, or other resources to a single machine.
  
  Pros of Vertical Scaling:
    Simplicity: It is often simpler to implement, requiring minimal changes to the existing architecture or application.
    Performance: Vertical scaling can lead to improved performance for individual tasks since the resources available to a single machine are increased.
    Cost-Effective for Small to Medium Workloads: For applications with relatively small or predictable workloads, vertical scaling can be cost-effective.
  
  How to Achieve Vertical Scaling?
    • Add More CPU: Increase the processing power by adding more powerful CPUs or additional processors to the existing hardware.
    • Add More Memory (RAM): Upgrade the amount of random access memory (RAM) to allow the server to handle more concurrent processes and data.
    • Increase Storage Capacity: Add more storage space, either by expanding existing drives or adding new ones, to accommodate growing data needs.
    • Upgrade to a More Powerful Server: Replace the current server with a more powerful one that can handle increased workloads.


   Why Not Vertical Scaling? What are The Problems We Still Have?
      - limitation

   Cons of Vertical Scaling
       - Limited Scalability: There is a practical limit to how much a single machine can be scaled vertically. Once the maximum capacity is reached, further scaling may require a more complex and expensive approach.
       - Downtime for Upgrades: Increasing resources often requires shutting down the server, which can result in downtime.
       - Higher Costs for High-Performance Needs: For very high-performance requirements, the cost of acquiring extremely powerful hardware may become prohibitive.
       - Single Point of Failure: If the single server fails, it can impact the entire system. Redundancy and failover mechanisms are crucial to mitigate this risk.

   When to Use Vertical Scaling?
      - Vertical scaling is suitable when the workload is well-suited to a single, powerful machine.
      - It's a viable option for applications with predictable resource needs.
      - In scenarios where simplicity and quick upgrades are crucial.

   Horizontal Scaling - Horizontal scaling, also known as scaling out, involves adding more machines or nodes or instance or server to a system to distribute the load and increase its overall capacity. Instead of making a single machine more powerful, horizontal scaling adds more machines to share the workload.

   
   Pros of Horizontal Scaling
      - Improved Scalability: Horizontal scaling allows a system to handle increased loads by adding more machines, providing a more flexible and scalable solution.
      - Cost-Effective: Horizontal scaling can be more cost-effective than vertical scaling, especially with the use of commodity hardware or cloud services where resources can be provisioned on demand.
      - Redundancy and Fault Tolerance: With multiple machines, the system becomes more resilient to failures. If one machine goes down, others can continue to handle requests.
      - Easy to Add Capacity: Adding more machines to a cluster is often a straightforward process and can be done without significant downtime.

   How to Achieve Horizontal Scaling

      - Load Balancing: Distribute incoming requests across multiple servers to ensure even utilization and prevent overloading a single server. Load balancers help achieve this by intelligently routing traffic.
      - Clustering: Create a cluster of machines that work together to handle requests. Clustering involves connecting multiple servers in a way that they act as a single system.
      - Containerization: Use containerization technologies like Docker or Kubernetes to deploy and manage applications in lightweight, portable containers. This facilitates the deployment of applications across multiple machines.
      - Auto-Scaling: Implement automated processes that dynamically adjust the number of instances based on demand. Cloud services often provide auto-scaling features to add or remove instances as needed.

   Monolithic is an anti pattern for scalablity
     - Issue with This Monolithic Architecture
            • Two Different IP Addresses
            • Two Different Databases
            • Two Different Storages
            • How client will decide which route to go?
      solution:
         - use load balancer
         - use single database (stateful service) have replicaset, sharding, indexing, partition
         - use single storage (stateful service)

      Cons of Horizontal Scaling
         - Complexity: Managing and coordinating multiple instances and ensuring they work seamlessly together can introduce complexity, especially in distributed systems.
         - Inter-Node Communication: In some cases, horizontal scaling may require efficient communication between nodes, which can be challenging to implement and maintain.
         - Data Consistency: Maintaining data consistency across multiple nodes can be complex, especially in distributed databases. Ensuring all nodes have the latest data can be a challenge.
         - Not Suitable for All Workloads: While horizontal scaling is effective for many scenarios, some applications may not benefit from it, especially those with high inter-process communication requirements.


      When to Use Horizontal Scaling
         - Horizontal scaling is suitable for applications with dynamic or unpredictable workloads.
         - It's effective for cloud-based architectures where resources can be provisioned and deprovisioned on demand.
         - When redundancy, fault tolerance, and improved scalability are critical.

      Scalability principales
         - Decentralization - One component is not responsible for all the work. If one component is responsible for all the work it is called Monolithic. Monolithic is an anti-pattern for scalability. Decentralization in scalability refers to the distribution of responsibilities and functions across multiple components or nodes within a system rather than centralizing them in a single entity. In a decentralized architecture, different parts of the system can operate independently, reducing bottlenecks and improving overall scalability.
                Key Aspects of Decentralization
                     - Distribution of Components: Instead of having a monolithic architecture, decentralization involves breaking down the system into smaller, independent components or services. Each component can operate autonomously.
                     - Data Distribution: In a decentralized system, data is often distributed across multiple nodes. This can involve partitioning data or replicating it across various servers to ensure that no single point becomes a performance bottleneck.
                     - Load Distribution: Workloads are distributed across multiple nodes or instances, ensuring that no single component bears the entire burden of incoming requests. Load balancing mechanisms play a crucial role in achieving this distribution.
               
               Advantages of Decentralization
                   - Improved Scalability: Decentralization allows a system to scale horizontally by adding more nodes to distribute the workload, providing a more efficient way to handle increased demand.
                   - Fault Tolerance: Since responsibilities are distributed, failures in one component do not necessarily affect the entire system. Decentralized systems are often more resilient to faults.
                   - Flexibility: Decentralized architectures are more flexible and adaptable to changes in workload. Adding or removing nodes can be done without disrupting the entire system.

         - Independence - Independence in scalability refers to the ability of components or modules within a system to operate autonomously without strong dependencies on each other. Each component can function independently, making it easier to develop, deploy, and scale.
               Key Aspects of Independence
                    - Loose Coupling: Components in an independent system are loosely coupled, meaning that changes to one component do not have a significant impact on others. This allows for more flexibility and ease of modification.
                    - Isolation of Concerns: Each component or service focuses on a specific task or functionality, and its internal workings are encapsulated. This isolation of concerns makes it easier to reason about and maintain individual components.
                    - Service-Oriented Architecture (SOA): Independence is often achieved through a service-oriented architecture, where different functions are provided by independent services that communicate through well-defined interfaces.

               Advantages of Independence
                    - Easier Maintenance: Independent components are easier to maintain and update since changes to one component are less likely to affect others.
                    - Scalability: Independent components can scale horizontally without causing cascading effects on the rest of the system. This supports the overall scalability of the system.
                    - Parallel Development: Teams can work on different components concurrently, leading to faster development cycles and efficient collaboration.

     Load Balancer:
           -A load balancer is a device or software application that distributes incoming network traffic across multiple servers or resources to ensure optimal utilization, prevent overload on any single server, and enhance the availability and reliability of a system or application. Load balancers are commonly used to manage traffic for web servers, databases, and other types of server farms.
      - Key Functions of Load Balancers
             - Single IP Address: Client don't need to know about IP addresses of all instances. Load balancer will keep track of it and expose a single IP address for client.
             - Traffic Distribution: Load balancers evenly distribute incoming network traffic, such as web requests, across multiple servers. This ensures that no single server bears the entire load and prevents any server from becoming a performance bottleneck.
             - Load Distribution: By distributing the workload across multiple servers, load balancers optimize resource utilization, preventing overloading on specific servers. This leads to improved performance and responsiveness of the overall system.
             - Scalability: Load balancers support horizontal scalability by easily integrating additional servers into the system. This enables the infrastructure to grow or shrink dynamically based on demand, enhancing the system's overall scalability.
             - Health Monitoring: Load balancers continually monitor the health and performance of individual servers. If a server becomes unavailable or experiences degraded performance, the load balancer can redirect traffic to healthier servers to maintain system stability.
             - Session Persistence: Some load balancers support session persistence, ensuring that requests from the same client are consistently directed to the same server. This is essential for applications that require maintaining session state, such as in e-commerce websites.
      
      - Use Cases of Load Balancers
            - Web Servers and Applications: Load balancers distribute incoming web requests across multiple servers, ensuring even utilization and preventing any single server from becoming a bottleneck. This is common in websites, online applications, and e-commerce platforms.
            - Application Servers in Multi-Tier Architectures: In multi-tier architectures, load balancers distribute traffic among application servers, helping to balance the load and improve the overall performance of the application
            - Database Servers: Load balancers can be used to distribute read queries among multiple database servers, optimizing the use of database resources and improving query response times.
            - Content Delivery Networks (CDN): CDNS use load balancers to distribute content to edge servers located in different geographical locations. This reduces latency and improves the speed of content delivery for users around the world.
            - File Servers and Storage Clusters: Load balancers distribute file requests across multiple file servers or storage clusters, preventing any single server from becoming overwhelmed and ensuring efficient data retrieval.
            - Mail Servers (SMTP, IMAP): Load balancers distribute email traffic across multiple mail servers, ensuring efficient handling of incoming and outgoing emails and preventing any single server from being overwhelmed.

      - HLB vs SLB
          - Hardware Based Load Balancer
               • Load distribution for L4 & L7
               • OSI Model
               • F5 Big IP 15000 series
                     - connection: 300 million
                     - Throughput: 320/160 Gbps
                     - RPS (L7): 10 million

         - Software Based Load Balancer
               • Load distribution L7
               • Features
                    - Content based routing
                    - Support SSL Termination
                    - Support Sticky Sessions
               • NGINX:
                    - Connections: 225K
                    - Throughput: 70 Gbps
                    - RPS: 3 million
      - Reverse Proxy:
            - A reverse proxy is a server that sits between client devices and a web server, forwarding client requests to the web server and returning the server's responses to clients. Unlike a forward proxy that sits between client devices and the internet, a reverse proxy is positioned on the server side to handle requests on behalf of the server. Reverse proxies provide several benefits, including improved security, load distribution, and caching.
         - Use Cases of Reverse Proxy
              - Request Forwarding: Reverse proxies forward client requests to backend servers, acting as an intermediary that relays requests on behalf of the clients.
              - Load Distribution: Similar to load balancers, reverse proxies distribute incoming requests across multiple backend servers to optimize resource utilization and improve the overall performance of the system. •
              - SSL Termination: Reverse proxies can handle SSL/TLS encryption and decryption, relieving backend servers of the resource-intensive task of managing secure connections. This enhances server performance and simplifies SSL certificate management.
              - Caching: Reverse proxies can cache static content, such as images, CSS files, and other assets, to reduce the load on backend servers and improve response times for frequently requested content.
              - Compression: Reverse proxies can compress content before sending it to clients, reducing bandwidth usage and improving the overall speed of content delivery.
              - Security: Acting as a barrier between clients and backend servers, reverse proxies enhance security by concealing the internal server structure, preventing direct access to backend servers, and mitigating certain types of attacks.
              - Web Acceleration: Reverse proxies can accelerate web applications by serving as a middle layer between clients and servers, optimizing content delivery, and reducing latency.
              - SSL Offloading: In addition to SSL termination, reverse proxies can offload SSL processing tasks from backend servers, freeing up server resources for handling application logic.

      - Load Balancer vs Reverse Proxy

         Feature              -            Load Balancer                 -                                                   Reverse Proxy
          Location                           Between clients and multiple servers.                                             On the server side, in front of backend servers.
          Functionality                      Primarily focuses on load distribution.                                           Manages communication between clients and servers, handles SSL termination, caching, and security.
          Use Cases                          Distributing traffic for performance and availability.                            Caching, SSL termination, security, and serving as a barrier between clients and servers.
          SSL/TLS Handling                   Can handle SSL/TLS offloading for multiple servers.                               Performs SSL/TLS termination, offloading encryption/decryption from backend servers.
          Caching                            Primarily focused on load distribution, may have limited caching capabilities.    Often incorporates caching to store and serve static content, reducing the load on backend servers.
          Client Communication               Directs client requests to appropriate backend servers.                           Communicates with clients on behalf of backend servers, protecting servers from direct exposure to the internet
          Type                               Software and Hardware                                                             Software

      - API Gateway
          - An API Gateway is a server that acts as an intermediary between an application and a set of microservices or APIs (Application Programming Interfaces). It serves as a single entry point for multiple APIs, handling tasks such as request routing, composition, security, and protocol translation. API Gateways are a crucial component in modern software architectures, providing a centralized point for managing and securing API interactions.
          Features of API Gateway
            - Request Routing: API Gateways route incoming requests from clients to the appropriate backend services or APIs based on predefined rules and configurations.
            - Authentication and Authorization: Ensures that only authorized users or applications can access the APIs by implementing authentication mechanisms such as API keys, OAuth, or other authentication protocols.
            - Request and Response Transformation: Modifies or transforms incoming requests and outgoing responses to ensure compatibility between client and server, such as converting data formats or handling versioning.
            - Rate Limiting and Throttling: Implements controls to limit the number of requests a client can make within a specified time frame, preventing abuse and ensuring fair usage of API resources.
            - Logging and Analytics: Captures detailed logs of API requests and responses, providing insights into API usage, performance, and potential issues. Analytics tools help in monitoring and optimizing API performance.
            - Caching: Stores and serves cached responses for frequently requested data, reducing the load on backend servers and improving response times for clients.
            - Security: Enforces security measures such as HTTPS, SSL/TLS termination, and protection against common security threats like SQL injection or cross-site scripting.
            - Monitoring and Health Checks: Monitors the health and availability of backend services, performing health checks and dynamically adjusting routing based on the status of the services.
            - API Versioning: Supports versioning of APIs, allowing clients to specify the desired version and ensuring backward compatibility as APIs evolve.
            - Distributed Tracing: Enables the tracking of requests across multiple microservices, providing insights into the flow of requests and helping identify performance bottlenecks.

      - Use Cases of API Gateway
          - API Management: Centralizes the management of APIs, making it easier to create, deploy, version, and retire APIs.
          - Security and Access Control: Enforces security policies, authenticates users, and ensures that only authorized clients can access specific APIs.
          - Request Transformation and Composition: Modifies or combines API requests to suit backend service expectations, optimizing the communication between clients and microservices.
          - Distributed Microservices Architecture: Serves as the entry point for client interactions with microservices, handling the complexity of multiple services and ensuring a unified API surface.
          - Legacy System Integration: Bridges the gap between modern applications and legacy systems, allowing newer applications to interact with older services through a standardized API.
          - Cross-Origin Resource Sharing (CORS): Facilitates secure cross-origin communication by enforcing CORS policies, allowing web applications to securely make requests to APIs hosted on different domains.
          - API Versioning and Evolution: Manages API versioning, ensuring backward compatibility and smooth transitions as APIs evolve over time.
          - Ingress Controller for Kubernetes: An API Gateway can serve as an Ingress Controller in Kubernetes, managing external access to services, handling SSL termination, and performing request routing based on domain
          - Third-Party Integration: Facilitates integration with third-party APIs and services, managing API keys, authentication, and data translation to ensure seamless communication between different services.
          - Microservices Communication: Serves as a central point for communication between microservices within an architecture, managing the complexities of service discovery, load balancing, and fault tolerance.

      - Popular API Gateways
           - Kong
           - Apache APISIX
           - Tyk
           - Ocelot
           - Amazon API Gateway
           - Azure

      - Replication
          - Replication in system design refers to the process of creating and maintaining multiple copies of data, components, or systems to improve reliability, fault tolerance, and performance. The purpose of replication is to ensure that a system remains available and functional even in the face of failures, outages, or increased demand.

          - Key Concept
             - Data Replication
                 - Copies of data are maintained across multiple locations, servers, or storage devices.
                 - This approach improves data availability, reduces latency, and enhances fault tolerance.
            - Component Replication
                 - Replicating entire components or services involves creating multiple instances of a service or application that can operate independently
            
            - Stateless Replication
               - Stateless replication involves replicating the functionality of a system or service without necessarily replicating its state. Each replica operates independently, and there is no shared state between replicas.
               - Key Characteristics:
                  - No Shared State: Stateless replicas do not share a common state. Each replica is responsible for managing its own state independently.
                  - Scalability: Stateless replication is often more scalable than stateful replication because each replica can operate independently, and there is no need for constant synchronization.
                  - Simplicity: Stateless replicas are typically simpler to implement, as there is no need to manage shared state or handle complex synchronization mechanisms.
                  - Use Cases: Stateless replication is common in scenarios where maintaining an identical state across replicas is not necessary, such as load balancing in web servers, content delivery networks (CDNs), and stateless microservices.
            - Stateful Replication
               - Stateful replication involves replicating both the data and the state of a system. In stateful replication, each instance or replica maintains the current state, and changes made to one replica are reflected in others.
               - Key Characteristics:
                  - Shared State: Stateful replicas share the same state, ensuring that each replica has an up-to-date copy of the system's data and state.
                  - Consistency: Maintaining consistency across replicas is critical in stateful replication. Changes made to one replica are propagated to others to ensure a coherent state.
                  - Challenges: Achieving stateful replication can be challenging, especially in distributed systems, as it requires mechanisms to synchronize and coordinate the state across replicas.
                  - Use Cases: Stateful replication is often used in scenarios where maintaining a consistent state across replicas is essential, such as in databases, distributed file systems, and certain types of distributed applications.
               - Stateful Replication - Web Application
                  - Not Recommended
                  - Low latency but lack of scalability and reliability
                  - When low latency is required such as Auth Session
                  - Manage sessions using
                        - Sticky session / session affinity
                        - Session clustering
               - Stateful Replication - Database
                     - Master Slave (Primary Secondary)
                        - Asynchronous
                            - Low latency writes
                            - Eventually consistent
                            - Data Loss
                            - Used for Read Replica
                        - Synchronous
                           - Consistent
                           - High latency writes
                           - Low write availability
                           - Used for Backup
                     - Master Slave (Peer to peer)
                          - Asynchronous
                              - Write conflicts
                              - High availability
                              - Use for multi regional service






•
•
•
•
•
•




Microservice Architecture | System Design and Application Architecture

   What is Microservice?
      => Microservices architecture is a software design approach where a complex application is decomposed into a set of small, independent, and loosely coupled services. Each service, known as a microservice, is designed to perform a specific business function and operates as an independent process. Microservices communicate with each other through well-defined APIs (Application Programming Interfaces) and can be developed, deployed, and scaled independently. This architectural style promotes modularity, flexibility, and the ability to evolve and update different parts of the application without affecting the entire system. The goal is to enable faster development, deployment, and maintenance, as well as improved scalability and resilience.
   
   Why should We Migrate to Microservices?
      => A growing team with conflicting feature requests is a common catalyst for transitioning to microservices. However, several other compelling reasons might push you towards this architectural shift:
   
   Agility and Faster Development
       • Independent deployments: Update and deploy individual services without affecting the entire application, enabling faster release cycles and continuous delivery.
       • Smaller codebases: Developers focus on smaller, well-defined service codebases, leading to quicker development and testing iterations.
       • Experimentation and innovation: Teams can experiment with new technologies and features in isolated services, minimizing risk and promoting innovation.
   
   Scalability and Resilience
       - Horizontal scaling: Scale individual services based on their specific needs, optimizing resource utilization and handling fluctuating demand effectively.
       - Fault isolation: Service failures are contained, preventing them from cascading and impacting the whole application, enhancing overall system resilience.
       - Increased uptime: Microservices architecture can lead to higher uptime and improved user experience due to its inherent resilience.
   
   Organizational Alignment and Ownership
       • Aligned teams and services: Service boundaries map to business capabilities, facilitating team ownership and fostering accountability.
       • Microservice teams: Dedicated teams own and manage specific services, leading to deeper expertise and domain knowledge.
       • Improved communication and collaboration: Teams collaborating on services fosters tighter communication and cross-functional understanding.
   
   Technology Flexibility and Modernization
       - Polyglot architecture: Each service can utilize the best fit technology stack, fostering technological innovation and avoiding vendor lock-in.
       - Modular modernization: Migrate specific services to newer technologies gradually, without needing to rewrite the entire application at once.
       - Leveraging cloud-native technologies: Microservices align well with cloud platforms and containerization technologies, enabling efficient resource utilization and deployment.
   
   Service (Individual):
     
     What is a Service?
        => A service in microservices architecture is an independent, deployable unit focused on delivering a specific business capability. It's like a well-defined task within a larger project, responsible for a particular function and operating with a high degree of autonomy.
      
     Key Characteristics
         - Focused functionality: Owns a specific domain or area within the application, providing clear and focused functionality.
         - Independent deployment: Can be developed, tested, and deployed independently without affecting other services.
         - Loose coupling: Communicates with other services through well-defined APIs, minimizing dependencies and preventing cascading failures.
         - Private data ownership: Ideally has its own data storage and controls its own data models for encapsulation and fault isolation.
         - Technology agnostic: Can be implemented using different languages and technologies.
   Move to microservice
      - Add an api Gateway
      - Separate authentication service
      - Migrate high value service
   
   Every Service can have Multiple Instance Running. How can API Gateway Know in Which Instance It Needs to Forward The Request?
     (Because Every Service Has Their Own IP Address)
     - Introduce load balancer
     - Introduce service discovery
     - Message Queues
   How to communicate between services ?
     There are Two Ways
        - Synchronous
            - HTTP / RESTful APIs
            - GRPC and Protocol Buffers
            - GraphQL
        - Asynchronous
            - Message Queues
            - Web Socket
   
   HTTP/RESTful APIs
      - Description: Synchronous communication often involves using HTTP/RESTful APIs for request-response interactions. A microservice sends an HTTP request to another microservice, and it waits for a response.
      - Used: 
      - Pros: Simplicity, ease of implementation, and real-time interactions.
      - Cons: Increased coupling between services, potential for cascading failures, and higher latency for some use cases.
   
   gRPC and Protocol Buffers
      - Description: gRPC is a remote procedure call (RPC) framework developed by Google. It uses Protocol Buffers as a serialization format for efficient and language-agnostic communication.
      - Used: Used for internal communication
      - Pros: Efficient binary serialization, support for bidirectional streaming, and strong contract definition.
      - Cons: Higher complexity compared to REST, potential for increased coupling.
   
   GraphQL
      - Description: GraphQL is a query language for APIs that enables clients to request only the data they need. It allows more flexible and efficient communication between services.
      - Used: 
      - Pros: Client-driven queries, reduced over-fetching of data, and flexibility in data retrieval.
      - Cons: Requires a specific skill set, complexity in implementing and securing.

   Message Queues
      - Description: Asynchronous communication involves the use of message queues (e.g., RabbitMQ, Apache Kafka). Microservices send messages to queues, and other microservices consume these messages asynchronously.
      - Used:
      - Pros: Loose coupling, improved fault tolerance, and better scalability for asynchronous processing.
      - Cons: Complexity in managing message ordering and potential delays in processing.
   
   Web Socket
      - Description: WebSocket provides full-duplex communication channels over a single, long-lived connection. It is suitable for scenarios requiring real-time bidirectional communication.
      - Used:
      - Pros: Real-time communication, reduced latency for specific use cases.
      - Cons: Requires support for bidirectional communication, potential for increased complexity.








System Reliability

Distributed System
   - More Likely to Fail
   - Failure Can Be
       - Partial
       - Independent
   
   Failures in Large Scale Distributed System
      - Definition: Large scale systems are typically distributed systems. Distributed systems are composed of multiple independent components that communicate and coordinate to achieve a common goal.
      - Characteristics: Large number of components: There are many different parts (software or hardware) that make up the entire system.
      - Large number of component instances: Each component may have multiple instances running to handle the system's load and provide scalability.

   Types of Failures
      
      - Partial Failures:
           - Definition: A partial failure occurs when only a specific part of the system malfunctions while the rest continues to operate.
           - Example: One service within the system stops working, but other services remain functional.
      - Independent Failures:
           - Definition: Failures that are independent won't impact other services or components in the system. The failure is contained within a specific service or instance.
           - Example: If one service fails, it doesn't affect the functionality of other services.
      
      - Single Point of Failures:
           - Definition: A single point of failure is a component or part of the system that, if it fails, can cause the entire system to fail.
           - Example: If a critical service that is required by many others fails, it becomes a single point of failure.
   
   Challenges in Large Scale Distributed System
      - 1. Increased Chance of Partial Failures: In large-scale systems, there are numerous components and instances, increasing the likelihood of individual parts experiencing failures.
      - 2. Cascading Effects: Partial failures can lead to complete system failures if they trigger a cascade of issues. For example, if a critical service fails, it might cause other dependent services to fail, creating a domino effect.
      - 3. Identifying Single Point of Failures: In complex distributed systems, identifying single points of failure can be challenging. It's not always straightforward to determine which component, if it fails, could bring down the entire system.
      - 4. Mitigation Strategies: Due to the complexity and potential for failures in large-scale distributed systems, it's crucial to implement strategies like redundancy, fault tolerance, and monitoring to mitigate the impact of failures.
   
   Reliability Engineering
      - Reliability
      - Availability
      - Fault Tolerance
   
   Reliability
      Reliability engineering is the systematic practice of predicting, preventing, and managing the probability of failure in systems and equipment. It's essentially the engineering discipline focused on making things function consistently and for their intended lifespans.
         - A system is said to be reliable if it can continue to function correctly and remain available for operations even in the presence of partial failures
         - It is measured as the probability of a system working correctly in a given time interval
      
      Reliability – Breakdown the Definition
         - Probability: This indicates the likelihood of the system functioning correctly. A perfectly reliable system would have a probability of 1 (100%), while a completely unreliable system would have a probability of 0 (0%).
         - Intended function: This refers to the specific purpose the system is designed to achieve. For example, a computer's intended function might be to run software applications, while a bridge's intended function might be to safely transport vehicles across a river.
         - Adequately: This means the system not only functions but also meets acceptable performance standards. For example, a computer might be considered reliable if it runs applications without crashing, but not if it runs them very slowly.
         - Specified period of time: This emphasizes that reliability isn't a universal concept. A system might be highly reliable for short periods but less reliable over longer durations. For example, a car might be reliable for a daily commute but might experience more failures over a long road trip.
         - Stated conditions: This highlights that reliability can be influenced by specific operating conditions. For example, a computer might be reliable under normal room temperature but might overheat and malfunction in a hot environment.
      
   Availability
      Availability in the context of computer systems and services refers to the probability that a system will be operational and accessible during a given period of time when it is supposed to be available for use. It is a critical metric in evaluating the reliability and performance of a system. Availability is often expressed as a percentage and is calculated using various methods, with two common approaches being time-based availability and request-based availability.

      Time Based Availability
         - Definition: Time-based availability is calculated based on the amount of time a system is operational (uptime) versus the total time, including periods of downtime.
         - Formula: Availability = Uptime / (Uptime + Downtime) ×100
         - Example: If a system is operational for 900 hours in a month and experiences 100 hours of downtime, the availability would be 900 / (900 + 100) ×100, resulting in 90% availability.
      
      Request Based Availability
         - Definition: Request-based availability is calculated by considering the number of successful requests versus the total number of requests made to the system.
         - Formula: Availability=Total Successful Requests / Total Requests × 100
         - Example: If a web service receives 9000 requests and successfully processes 8500 of them, the availability would be 8500 / 9000 * 100, resulting in approximately 94.44% availability.
      
      Key Points
         - Downtime Tolerance: Availability does allow for some downtime, but the system is expected to recover quickly. It's about ensuring that when a system does go down, it doesn't stay down for an extended period.
         - Measuring Reliability: Availability is a key metric in assessing the reliability of a system. High availability indicates that the system is dependable and accessible, meeting user expectations.
         - Trade-offs: Achieving high availability often involves trade-offs, as increased redundancy and fault tolerance mechanisms may be necessary, which can impact cost and complexity.
         - Continuous Monitoring: To maintain and improve availability, systems are often subject to continuous monitoring, performance analysis, and proactive measures to identify and address potential issues before they lead to downtime.
      
      High Availability
         - High Availability refers to the design and implementation of systems or services with the goal of ensuring a high level of operational performance and accessibility. Achieving high availability involves minimizing downtime and maintaining continuous operation, even in the face of failures or disruptions. The points mentioned highlight various aspects associated with high availability:
            - Cost of Availability: While high availability isccrucial, it comes with associated costs. Implementing redundancy, fault tolerance, and other measures to achieve high availability often involves additional infrastructure, technology, and operational expenses.
            - Business Impact: The level of availability required should align with the business impact of downtime. Critical systems with high business impact, such as e-commerce platforms or financial services, typically demand higher availability levels.
      Key Points
         - Trade-offs:
             - Factors Impacting High Availability:
                  - a. New Features: Achieving high availability may involve trade-offs with the introduction of new features. Resources allocated to maintaining availability might limit the resources available for developing and releasing new functionalities.
                  - b. Operational Costs: The operational costs associated with maintaining high availability, such as redundant infrastructure and monitoring systems, should be considered.

            - Rollout of New Features:
                  - a. Strategic Use of Downtime: Systems with high availability often use the permitted downtime specified in Service Level Agreements (SLAs) or Service Level Objectives (SLOs) for planned activities, such as the rollout of new features or updates.
                  - Example: Scheduled maintenance or updates can be strategically performed during periods of lower demand to minimize the impact on users.
                  - b. Challenge: Introducing new features can potentially cause disruptions, even in highly available systems. Ensuring a smooth rollout without affecting ongoing operation is a complex challenge.
      
      The Nines' of Availability
         - If availability is 99.00% available, it is said to have "2 nines" of availability, and if it is 99.9%, it is called "3 nines", and so on.

         the nines availability table
      
   Fault Tolerance
      Fault tolerance is a set of techniques and strategies designed to improve the availability and/or reliability of a system by enabling it to continue functioning even when certain components or subsystems experience faults.

      - Fault-tolerant systems are designed to automatically:
         - Detect Partial Failures
         - Handle Partial Failures
         - Recover from Partial Failures
      - Detect Partial Failures: Identify and recognize when a component or subsystem is experiencing a partial failure.
      - Handle Partial Failures: Implement mechanisms to ensure that the system can continue operating despite the presence of partial failures.
      - Recover from Partial Failures: Initiate recovery processes to restore full functionality after a partial failure has occurred.

   Design Fault Tolerance
      - Redundancy
      - Fault Detection
      - Recovery

      -Redundancy
         - Redundancy refers to the duplication of critical components, functions, or resources within a system with the aim of increasing reliability and ensuring continued operation in the event of a failure or disruption.
         - Replication / Duplication of critical components or functions of a system in order to increase its reliability
         - A secondary capacity is kept ready as backup, over and above the primary capacity, in case the primary is not available
         - Ex. Backup tiers of cars, Power bank

      - Example - 3 Tier Application

         - Application Tier:
           - Duplicate application servers are deployed to handle client requests. If one server fails, the other servers can still process the requests.
           - Load balancers distribute incoming requests across multiple application servers to ensure efficient resource utilization and fault tolerance.
         
         - Database Tier:
           - The database is replicated to have multiple copies of the data. This can be achieved through techniques like master-slave replication or multi-master replication.
           - If one database server fails, the other servers can continue serving read and write requests, ensuring data availability and minimizing downtime.
         
         - Storage Tier:
           - Redundant storage devices or disks are used to store data. RAID (Redundant Array of Independent Disks) configurations, such as RAID 1 (mirroring), can be employed to duplicate data across multiple disks.
           - If one disk fails, the redundant disk(s) can take over, preventing data loss and maintaining system availability.
         
         - Redundancy Types
            - Active Redundancy - Hot Spare
            - Passive Redundancy - Warm Spare
            - Cold Redundancy - Spare (Backup)

         - Active Redundancy
            - Definition: In active redundancy, all nodes within a system actively participate in processing tasks simultaneously. This setup is also known as a "hot spare" configuration.
            - Characteristics:
            - Simultaneous Processing: All nodes are actively involved in handling the workload.
            - Highest Availability: This configuration is ideal for maximizing availability since any node can take over if another fails.
            - Example: In a cluster of servers, each server actively processes requests. If one server fails, another is ready to seamlessly pick up the workload, ensuring uninterrupted service.
         
         Passive Redundancy
            - Definition: Passive redundancy, or "warm spare," involves having standby nodes that are not actively processing tasks. However, they are ready to take over in case of a failure.
            - Characteristics:
               - Selective Processing: Only active nodes are processing tasks; standby nodes remain ready.
               - Quick Recovery: While not actively contributing to processing, standby nodes can quickly assume the workload if needed.
            - Example: In a database cluster, one server is actively handling queries while other remains on standby. If the active server fails, the standby server can quickly take over to minimize downtime.
         
         Cold Redundancy (Not for where used load balancer or server less)
            - Definition: Cold redundancy, or having spare nodes as backups, involves keeping additional nodes in a dormant state until needed. These nodes are brought online only in the event of a failover.
            - Characteristics:
                - Inactive Nodes: Spare nodes remain offline until activated during a failover.
                - Not High Availability: This setup is not inherently designed for high availability since there is a delay in bringing the backup nodes online.
            - Example: In a network, backup routers may be kept in a powered-off state until the primary router fails. In case of failure, the backup router is activated to restore network functionality.
         
         SPOF
           - A single point of failure refers to a component or a part of a system that, if it fails, will cause the entire system to fail. This means that the failure of one critical component can disrupt the entire system's functionality. It is essential to identify and address single points of failure to ensure the reliability and availability of the system.
           - For example, let's consider a web application that relies on a single database server to store and retrieve data. If this database server becomes unavailable due to a hardware failure or software issue, the entire application will be affected, and users won't be able to access their data or perform any operations. In this case, the database server acts as a single point of failure.
         
         Redundancy for
           - Stateful Component
           - Stateless Component
           - Load Balancer
           - Datacenter Redundancy
         
         - Fault Detection
             Fault Models
                 - Response Failure: A server fails to receive or respond to incoming messages
                 - Timeout Failure: A server response duration is longer than timeout duration
                 - Incorrect Response Failure: A server's response is incorrect
                 - Crash Failure: A server halts but it working correctly until it halts (Bugs and crash)
                 - Arbitrary Response Failure: Response is incorrect because of security compromised
         
             Health Checks
               - Health checks are a mechanism used to monitor the health and status of servers or components within a system. They help detect faults or failures in a timely manner and allow for proactive recovery or mitigation actions.
               There are mainly two types of health check techniques,
                  -Ping Based
                  -Heartbeat Base
               
             Ping Based Health Check
                 - An external monitoring service periodically sends ping requests to a server. If the server fails to respond to the ping requests within a specific time frame, it indicates a potential fault or unavailability of the server. This type of health check is commonly used to monitor the network connectivity and basic availability of a server.
             Heartbeat Based Health Check
                 - In an internal cluster monitoring setup, servers within a cluster exchange heartbeat signals at regular intervals. If a server stops sending heartbeats or fails to receive heartbeats from other servers within the expected timeframe, it indicates a potential fault or unavailability of the server. This type of health check helps ensure the availability and proper functioning of servers within a cluster.
             
             External Monitoring Service
                - Health check service generates
                    - Alerts for recovery
                    - Events - for scaling
                - Application Health Checks
                    - HTTP Response
                    - TCP Response
                - Periodic Health Checks
                    - Response Code
                    - Response Time
                    - Number of Retries
         
         - Recovery
            - Stateless Recovery
                 - Hot Standby
                 - Warm Standby
            - Stateful Recovery
                 Database Failover
                   - Master Slave
                   - Master Master
                 
                 Cache Failover
                   - Redundant Replica
                   - Synchronized
                 
                 Load Balancer
                   - Redundant Load Balancer
                   - Updating DNS using Virtual IP
         - System Stability
            Timeouts
               Client Components
                  - User interface: Timeout mechanisms are essential in user interfaces to prevent applications from freezing or becoming unresponsive when waiting for a response from a service or server. For example, if a web page is loading data from a server, a timeout ensures that the user interface remains interactive even if the server
                  - Service clients: In the context of service-oriented architectures, client components often communicate with backend services. Timeouts in service clients ensure that if a response from the service takes too long, the client doesn't wait indefinitely, improving overall system responsiveness.
               Timeouts prevents call to integration points from becoming blocked threads
                  - Dependency Failures: In distributed systems, components often depend on external services or resources. If one service fails or experiences a prolonged delay, it can lead to a cascading effect, affecting downstream components.
                  - Timeouts as Safeguards: Timeouts act as safeguards by limiting the time a component is willing to wait for a response. If the response is not received within the specified time frame, the calling component can take appropriate action, such as retrying, logging an error, or providing a fallback response. This helps prevent a single failure from causing a chain reaction of failures.
            
            Retries
               For transient failures
                  - Not for permanent failures
               For system errors
                  - For example one instance get down
                  - Not for application errors
               Retries with exponential back-off is a technique that retries an operation, with an exponentially increasing wait time, up to a maximum retry count has been reached
               Return HTTP 503
                  - Client can decide when to callback again
            
            Circuit Breaker (failed dectection)
               - The circuit breaker is a design pattern used to detect failures and encapsulates the logic of preventing a failure from constantly recurring during maintenance, temporary external system failure, or unexpected system difficulties.
                   - Deliberate service degradation when a system is under stress and a problem is detected
                   - Process
                       - Keep track of success and failures
                       - In the event of too many failures, fallback to
                           - Default values
                           - Cached values
                           - Error messages
                       - Resume when stress dissipates
                  - Closed: When everything is normal, the circuit breakers remain closed, and all the request passes through to the services as normal. If the number of failures increases beyond the threshold, the circuit breaker trips and goes into an open state.
                  - Open: In this state circuit breaker returns an error immediately without even invoking the services. The Circuit breakers move into the half-open state after a certain timeout period elapses. Usually, it will have a monitoring system where the timeout will be specified.
                  - Half-open: In this state, the circuit breaker allows a limited number of requests from the service to pass through and invoke the operation. If the requests are successful, then the circuit breaker will go to the closed state. However, if the requests continue to fail, then it goes back to the open state.







Application Security
