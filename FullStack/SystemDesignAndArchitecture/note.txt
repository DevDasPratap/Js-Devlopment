Application Architecture:
    - Which Architecture should we follow?
    => it depend on bussiness requirment
    - Architectures:   
        - Monolithic Architecture: Monolithic architecture refer to a software application design all components are tightly coupled and integrate into a single, self -contained unit.(single tier architecture)
                                   - key characteristics
                                      - single codebase
                                      - centralized deployment
                                      - shared database
                                      - tight coupling
                                   - advantage
                                   - disadvantage
                                   - suitable for:
                                      - simple application: where the functionality is limited and unlikely to changes significatly
                                      - small team: where resource are limited
                                      - POC: where the focus is on validating a concept before investing in a complex architecture

            - 2-Tier Architecture:in this software architecture pattern that divides the application into two layers
                 - 1. presentation tier: this layer handle the user interface and intracts with the user directly
                 - 2. data tier (server): this layer handle bussiness logic and data store and processing
                 - advantage: improve scalablity, enhance mentainablity, increse security, flexible thechnical choose, simplify deployment.
                 - disadvantage: increse complex, additional network trafic, signle point failour(if server fails the entier application unavaliable)
                 - suitable for:
            - N Tier Architecture: also known as multi tier architecture is a software architecture pattern that separate the application into multiple layer.
                        - advantage: improve scalablity, enhance mentainablity, increse security, simplify deployment, thechnical choose
                        - disadvantage: increse security, additional network trafic, increse debugging defficulty
                        - suitable for: large web application with complex bussiness logic, enterprize application that require.
            - Modular Monolithic Architecture: a Modular Monolithic architecture is a software development approch..
                       - single codebase
                       - independent modules
                       - loose coupling
                       - shared resources
                       - advantage
                       - disadvantage
                       - suitable for  
            - Microservice Architecture: Microservice architecture is a software development approch that structure an application as a collection of small, independent, and loosely coupled service. each service is self-contained and focuses on single bussiness capability.services communicate well defined apis
                  - key characteristics
                    - independent service
                    - loose coupling
                    - lightweight technology
                    - decentralize data
                - advantage: 
                    - improve scalablity
                    - enhance agility
                     - increse fauilt tolarance
                     - simplify mentainablity
                     - flexible technology choose
                - disadvantage: 
                    - increse complex
                    - distribute debugging
                    - infrastructure overhead
                    - network latency
                    - data consistency
                - suitable for: large and complex application that require scalablity, application with evolving requirment and frequent updates. 
            - Event-Driven Architecture: is a software design pattern that focuses on events significate occurance or changes within a system - to drive application logic and communication. instead of traditional request, response intractions, events trigger actions and updates across diffrent services.
               - main key:
                  - events
                  - events producer
                  - events comsumer
                  - events broker
                - advantage:
                   - scalablity
                   - real time processing
                   - async communicate
                - disadvantage: 
                    - increse complex
                    - monitoring and tracing
                    - data consistency
                    - debugging difficult
                - suitable for
            - Cloud Native Architecture: this is refers to methology for designing and building, software application, specifically for development and operation in the cloud computing envionment
                - key
                   - Microservice
                   - containers
                   - continous delivery
                - advantage
                - disadvantage
                - suitable for
            - Serverless Architecture: is a cloud computing execution model where the cloud provider
              - key
                - no server management
                - pay-per use
                - scalablity
                - event drivent
                - Microservice
                - stateless
              - advantage
              - disadvantage
              - suitable for   
            - How to choose the perfect Architecture?
              => there is no defination for perfect architecture, start with minimum effort and budget  
            - What if I want to make a wise decision? 
               - Focus on the following factors:
                  - project requirment
                     - complexity
                     - scalablity
                     - performance
                     - mentainablity
                     - security
                     - change frequent
                  - team expertices
                     - team experiance
                     - require skill set
                  - timeline
                     - respected deadline
                  - budget
                     - development
                     - deployment







improve the application performance:

What is performance?
=> application performance refers to the 'how well' your software application operations in terms of various metrics. it encompasses servral key aspects, including:
   - responsiveness
   - stability
   - scalablity
   - resource utilization
   - user experiance

for a backend application
   - measure of how fast or responsive  a system in under
      - a given workload
          - backend data
          - request valume
      - a given hardware
          - kind
          - capacity

how to spot a performance problem?
=> spotting a performance problem require vigilance and undersatand of your system normal behavior.
    - monitoring metrics
       - incresed response time: pages taking longer to load, delays in progressing inputs, or unresponsive application are telltale signs.
       - higher error rate: frequent crashes, failed transactions, or application error indicate underlying performance issues.
       - resource utilization: spikes in cpu usage, memory exhaustion or high network brandwidth consumption can point to bottlenecks.
       - log analysis: reviewing system logs may reveal specific error warnings or performance related events.
    - user feedback
       - complain about slowness: users reporting sluggish performance, unresponsive application.
       - incresed support tickets: A rise in supports tickets related to performance problem further reinforces the need for invesigatiom
    - proactive checks
       - load testing
       - performance profiling
       - benchmarking
    - visual indicator
       - slow loading animations or progress bars
       - log in user interface

every performance problem is the result of some queue building somewhere

where does a queue can build up ?
 - network:
    - incresed latency: ping times become higher, uploads and download take longer, and timeouts occur more frequently.
    - packet loss: data packets further get dropped, leading to incomplete information and retransmission requests, further extending wait times.
    - full send/recive buffer: Queues on the network card or operating system fill up, preventing data from being sent or received efficiently.
 - database:
    - slow query execution:  Long wait times for database queries to complete, impacting page loading times and user actions.
    - database connection pool execution: All available database connections are in use, preventing new requests from being served, leading to user wait times.
    - disk i/o bottleneck: Queues for reading and writing data to the disk build up, impacting database operations and application responsiveness.
 - operating system
    - High cpu utilization
    - excessive memory usage
    - long progress wait times
 - application code
    - ineffcient algorithms
    - redundant calculation
    - code deadlocks

queue build up root cause
  - badly written code
    - ineffcient algorithms, unnecessary calculation, or poor memory management can all contribute to slow processing.
  - external dependencies
   - relying on slow external service or resource intensive libraries can also add to the processing time.
  - database access
    - multiple requests needing to connect to a database will form a queue waiting for available connections.
  - serial execution in code
    - if tasks are coded to run sequentially one after the other, any bottleneck in the chain will slow down the entier processs

Limited resource capacity: This is essentially a physical limitation of the system. For example, having only one CPU means that only one task can be actively processed at a time. Any incoming requests beyond that will have to wait for their turn, forming a queue.
Other limited resources can also create bottlenecks, such
Memory: Insufficient memory can force the system to

swap data to disk, significantly slowing down processing. Network bandwidth: Limited network bandwidth can create queues for data transfer, impacting download and upload speeds.

By adopting this queue-based perspective, we can effectively spot performance issues early on, diagnose their root causes, arid implement targeted solutions to keep our system running smoothly.

Inefficient slow processing: This refers to a situation where the code responsible for handling tasks takes too long to execute. This can happen due to various factors, like:
Badly written code: Inefficient algorithms, unnecessary calculations, or poor memory management can all contribute to slow processing.
Serial resources access: This happens when accessing & resource requires tasks to be executed one after another, creating a queue as requests wait for their turn. Examples include:
Database access: Multiple requests. needing to connect to a database will form
a queue waiting for available connections Shared files: If only one thread can access a file at a time, others will have to wait in line.
External dependencies: Relying on slow external services or resource-intensive libraries can also add to the processing time.
Serial execution in code: If tasks are coded to run sequentially, one after the other, any bottleneck in the chain will slow down the entire process.

Note
We should always try to avoid building queues when designing a new system or find where the is queue building in an existing system.

performance principales:
   - efficiency: reduce response time on a single request
       - Efficient Resource Utilization (System resources utilization)

          - a 10-Memory, Network, Disk
          - b CPU
       - Efficient Logic
          - a Algorithms
          - b DB Queries
       - Efficient Data Storage
          - a Data Structures
          - b DB Schema
       - Caching
   - concurrency: reduce response time on a concurrent request
       - Hardware (must have concurrency support
           - multiple core)
           - Software (we need to code our software that can utilize multiple core)
              - a Queuing (this queue is not going to block requests)
              - b. Coherence (being logical)
   - capacity: increse hardware improve performance
               - CPU
               - Memory
               - Disk
               - Network
   
   performance measurement metrics
     - latency
       - Measured in time : Focuses on how quickly the system responds to a request.
       - Affects user experience: Focuses on how quickly the system responds to a request.
       - Desired State: low as possible to keep users satisfied and maintain smooth interaction.
     - throughput
        - Measured in Request Per Unit of Time: Indicates the system's ability to handle concurrent requests.
        - Affects number of users supported: Focuses on how quickly the system responds to a request.
        - Desired State: Greater than the request rate to avoid backlogs and maintain responsiveness under peak load.
     - errors
       - Percentage of requests resulting in errors: Represents the system's reliability and correctness.
       - Affects functional correctness:Errors can disrupt functionality and lead to user frustration.
       - Desired State: None, aiming for zero errors to ensure accurate and reliable operation.
     - resource saturation
       - Measured in Percentage: Indicates how busy system resources (CPU, memory, network) are at any given time,
       - Affects hardware capacity required: Over-saturation can lead to performance bottlenecks and potential hardware upgrades.
       - Desired State: Efficient utilization of all resources, avoiding both underutilization (waste) and overutilization (bottlenecks)


 - Tail Latency
   Tail latency refers to the response times of the slowest requests within a certain set, typically measured at the 99th or 99.9th percentile. It provides a more comprehensive picture of performance compared to focusing solely on average latency, which can be misleading if there are a few outliers with significantly higher response times.
      - Tail latency is an indication of queuing of requests. Gets worse with higher workloads
      - Average latency hides the effects of tail latency.
   - Additional Considerations: 
      - It's crucial to define realistic target values for each metric based on your specific system, user base, and workload characteristics.
      - Monitor these metrics continuously to identify trends, bottlenecks, and potential issues before they impact users.
      - Optimize your system based on these metrics, focusing on reducing latency, improving throughput, minimizing errors, and optimizing resource utilization.
      - Remember, these metrics often interact and influence each other. Optimizing one might affect others, requiring a holistic approach to performance optimization.
  - Serial Request Latency
    - Network Connection: There are two types of connections -
        - Internet (Public)
        - Intranet (Private)
        - Intranet is a much reliable connection and faster
    - TCP Handshake
        - Client   <----> Server
        - State changes to SYN-SENT
        - SYN seq: 100
        - State changes to ESTABLISHED
        - SYN-ACK
        - seq: 200
        - ack:
        - ACK
        - seq: 101
        - ack
        - State changes to
        - State changes to SYN-RECEVED
    - TLS Handshake
        - Client
        - Server
        - Establish TCP Connection
        - SYN
        - SYN ACK
        - Client Hello
        - SSL/TLS Handshake
        - Server Hello Certificate
        - Certificate Vers
        - Server Finished
        - Encrypted Application Data
        - HTTP GET
        - HTTP Response
    - Network Latency
      - Data Transfer Latency - Every computer is connected via wire. It takes some time travel data from point A to point B. That is called data transfer latency.
      - TCP Connection - Creation or Establish a connection also has some latency
      - SSL/TLS Connection - Much more overhead
    - Network Latency - Approaches
  - TCP Connections:
    - Connection Pool (DB App Server): Reusing existing connections instead of establishing new ones for every request can significantly reduce connection setup time and latency.
    - Persistent Connections (Client - API Gateway):
    - HTTP/1.1 automatically maintains persistent connections, eliminating the need for handshakes in subsequent requests, improving latency.
    - SSL Session Caching: Reusing established SSL sessions minimizes the overhead of handshakes, especially for HTTPS connections, saving bandwidth and reducing latency.
  - Data Transfer.
    - Session / Data Caching: Caching frequently accessed data at the client or intermediate servers (eg. API Gateway) avoids fetching it from the origin server, reducing round-trip time and latency.
    - Static Data Caching: Cache static content like logos, scripts, and images close to the user (eg, CDN) to minimize fetching time and improve page load speed. Data Format & Compression: Choosing efficient data formats (eg. JSON over XML) and compression techniques can significantly reduce data size, leading to faster transmission and lower latency.
    - Binary Data-gRPC: Using protocols like gRPC optimized for binary data transfer avoids unnecessary text parsing and serialization/deserialization, further reducing latency for transferring large data sets.

  - Network Latency - Approaches
    -Additional considerations:
       - Minimize HTTP requests: Combine resources into fewer requests to reduce round-trip times and network overhead.
       - Optimize DNS resolution: Use a Content Delivery Network (CDN) with strategically placed caching servers to minimize DNS lookup times..
       - Monitor and analyze: Continuously monitor network performance metrics like latency, throughput, and packet loss to identify bottlenecks and areas for further optimization.

   - Memory Access Latency
      - Finite Heap Memory (If we reach the limit the app will crash)
      - Large Heap Memory (The application may occupy a large number of memory)
      - GC (garbage collector) Algorithm.
      - Finite Buffer Memory (on DB)
   - Minimize Memory Access Latency
      - 1. Avoid Memory Bloat: This is the crucial principle. Reduce the overall amount of data your application holds in memory at any given time.
      - 2. Weak/Soft References: Utilize these reference types to indicate data that can be reclaimed by the garbage collector if resources become scarce. This helps prevent unnecessary data being held in memory permanently.
      - 3. Multiple Smaller Processes: Splitting your application into smaller, focused processes can help reduce individual memory footprints.
      - 4. Garbage Collection Algorithm: Choosing an efficient GC algorithm and tuning its parameters can significantly impact latency. Opt for algorithms with low pauses and efficient memory reclamation to minimize overhead during GC cycles.
      - 5. Batch Processes: Process data in batches instead of single items. This can improve memory efficiency by reducing context switching and increasing data locality within cache lines.
      - 6. Constantly run with main process: if possible, avoid creating and destroying the main process frequently. This minimizes memory allocation and deallocation overhead, reducing latency associated with these actions.
      - 7. Database Normalization: Implement proper database normalization to avoid redundant data storage. This reduces the memary footprint on the database side and minimizes unnecessary data transfers to the application, improving overall access times.
      - 8. Computer Over Storage: Prioritize processing data in memory where possible instead of relying on disk storage. This significantly reduces access latency compared to fetching data from slower disks.
      - 9. Don't store unnecessary data: Carefully analyze your data needs and avoid storing information that can be easily calculated on demand. This frees up valuable memory space and access to truly essential data

   -Disk Access Latency
     -Disk IO is very slow.
        -Web Content Files
        -Logging
        -DB Disk Access (Very Very Important)

   -Minimize Memory Access Latency
        -1. Sequential & Batch IO: Disks are significantly faster at writing and reading data sequentially compared to random access. Batching operations together and writing sequentially can dramatically improve performance.
        -2. Asynchronous Logging (Worker Thread): Offload logging to a separate worker thread to avoid blocking the main thread and slowing down critical operations.
        -3. Web Content Caching: Cache static content like images, scripts, and stylesheets closer to the user (e.g., CDN) to minimize disk access on the origin server.
        -4. Reverse Proxy / Load Balancer: Utilize these tools to distribute traffic across multiple servers, reducing the burden on individual disks and preventing bottlenecks. Additionally, reverse proxies can cache content further improving access times.
        -5. Separate hosting of dynamic and static data: Host dynamic content, that frequently changes, on separate servers or storage systems from static content.
        -6. Page Cache, Zero Copy: Utilize the operating system's page cache to keep frequently accessed data readily available in memory, minimizing disk access for repeated requests.
        -7. Query Optimization: Efficiently written database queries minimize disk access by retrieving only the necessary data.
        -8. Data Caching: Cache frequently accessed database results within the application to further reduce disk access for repeated requests.
        -9. Schema Optimization: Design your database schema with efficient data layout and appropriate indexing to minimize disk seeks and improve query execution speed.
        -10. Higher IOPS, RAID, SSD Disk: Consider hardware upgrades like disks with higher IOPS (Input/Output Operations per Second) or RAID configurations for improved data access parallelism. SSDs offer significantly faster access times compared to tradit HDDs.


   -CPU Latency
      -Inefficient Algorithms (Developer can handle)
      -Context Switching
        - In computing, a context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point, and then restoring a different, previously saved, state.
CPU
↓
Interrupt or system call
↓
Save state into PCB1
↓
Reload state from PCB2
↓
Interrupt or system call
↓
Save state into PCB2
↓
Reload state from PCB1


Process P1
↓
Executing
↓
↓
Idle
Executing


Process P2
↓
Idle
↓
Executing
↓
Idle

  - Minimize CPU Latency
     - Efficient Algorithms
     - Efficient Queries
     - Batch/Async IO
     - Single Threaded Model
     - Thread Pool Size
     - Multi-Process in Virtual Environment

    - Latency Costs
    - Concurrent Request: It requires knowledge of Operating System and Multithreading
    - capacity: increse hardware improve performance
    - caching: we will have a separate sessions for that










Scalablity of Application

we optimization the application performance to make our client happy

current system performance
    - req-res latency 150 ms per request average, 350 ms tail latency, throughput - 1000 concurrect request per Second

whole day the application performance really well, howerver during the peak hour the system gets shut down.

what we can do next ?
we need to scale our system

scaling in system design
  - Scaling in system design refers to the ability of a system to handle increased load or demand while maintaining or improving its performance. Scaling is an essential aspect of system design, especially in the context of distributed and cloud-based applications, where workloads can vary over time and may experience rapid growth. The goal of scaling is to ensure that a system can adapt to changing circumstances, such as increased user traffic or growing datasets, without compromising its responsiveness or reliability.

Performance vs Scaling

Performance:
  • Low Latency
  • High Throughput
      • Concurrency
           • Single Machine (Multi Threading)
           • Multi Machine (Multi Threading + Multi Processing = Distributed Processing)
      • Capacity
   • Capacity

Scalability
   • Subset of Performance
   • High Throughput
       • Ability of a system to increase its throughput by adding more hardware capacity
   • Scale both up and down

There are two types of scaling
   - vertical
   - horizontal

  Vertical Scaling - Vertical scaling, also known as scaling up or scaling vertically, refers to the process of increasing the capacity of a single server or resource in order to handle a larger load or improve performance. This is typically achieved by adding more CPU, memory, storage, or other resources to a single machine.
  
  Pros of Vertical Scaling:
    Simplicity: It is often simpler to implement, requiring minimal changes to the existing architecture or application.
    Performance: Vertical scaling can lead to improved performance for individual tasks since the resources available to a single machine are increased.
    Cost-Effective for Small to Medium Workloads: For applications with relatively small or predictable workloads, vertical scaling can be cost-effective.
  
  How to Achieve Vertical Scaling?
    • Add More CPU: Increase the processing power by adding more powerful CPUs or additional processors to the existing hardware.
    • Add More Memory (RAM): Upgrade the amount of random access memory (RAM) to allow the server to handle more concurrent processes and data.
    • Increase Storage Capacity: Add more storage space, either by expanding existing drives or adding new ones, to accommodate growing data needs.
    • Upgrade to a More Powerful Server: Replace the current server with a more powerful one that can handle increased workloads.


   Why Not Vertical Scaling? What are The Problems We Still Have?
      - limitation

   Cons of Vertical Scaling
       - Limited Scalability: There is a practical limit to how much a single machine can be scaled vertically. Once the maximum capacity is reached, further scaling may require a more complex and expensive approach.
       - Downtime for Upgrades: Increasing resources often requires shutting down the server, which can result in downtime.
       - Higher Costs for High-Performance Needs: For very high-performance requirements, the cost of acquiring extremely powerful hardware may become prohibitive.
       - Single Point of Failure: If the single server fails, it can impact the entire system. Redundancy and failover mechanisms are crucial to mitigate this risk.

   When to Use Vertical Scaling?
      - Vertical scaling is suitable when the workload is well-suited to a single, powerful machine.
      - It's a viable option for applications with predictable resource needs.
      - In scenarios where simplicity and quick upgrades are crucial.

   Horizontal Scaling - Horizontal scaling, also known as scaling out, involves adding more machines or nodes or instance or server to a system to distribute the load and increase its overall capacity. Instead of making a single machine more powerful, horizontal scaling adds more machines to share the workload.

   
   Pros of Horizontal Scaling
      - Improved Scalability: Horizontal scaling allows a system to handle increased loads by adding more machines, providing a more flexible and scalable solution.
      - Cost-Effective: Horizontal scaling can be more cost-effective than vertical scaling, especially with the use of commodity hardware or cloud services where resources can be provisioned on demand.
      - Redundancy and Fault Tolerance: With multiple machines, the system becomes more resilient to failures. If one machine goes down, others can continue to handle requests.
      - Easy to Add Capacity: Adding more machines to a cluster is often a straightforward process and can be done without significant downtime.

   How to Achieve Horizontal Scaling

      - Load Balancing: Distribute incoming requests across multiple servers to ensure even utilization and prevent overloading a single server. Load balancers help achieve this by intelligently routing traffic.
      - Clustering: Create a cluster of machines that work together to handle requests. Clustering involves connecting multiple servers in a way that they act as a single system.
      - Containerization: Use containerization technologies like Docker or Kubernetes to deploy and manage applications in lightweight, portable containers. This facilitates the deployment of applications across multiple machines.
      - Auto-Scaling: Implement automated processes that dynamically adjust the number of instances based on demand. Cloud services often provide auto-scaling features to add or remove instances as needed.

   Monolithic is an anti pattern for scalablity
     - Issue with This Monolithic Architecture
            • Two Different IP Addresses
            • Two Different Databases
            • Two Different Storages
            • How client will decide which route to go?
      solution:
         - use load balancer
         - use single database (stateful service) have replicaset, sharding, indexing, partition
         - use single storage (stateful service)

      Cons of Horizontal Scaling
         - Complexity: Managing and coordinating multiple instances and ensuring they work seamlessly together can introduce complexity, especially in distributed systems.
         - Inter-Node Communication: In some cases, horizontal scaling may require efficient communication between nodes, which can be challenging to implement and maintain.
         - Data Consistency: Maintaining data consistency across multiple nodes can be complex, especially in distributed databases. Ensuring all nodes have the latest data can be a challenge.
         - Not Suitable for All Workloads: While horizontal scaling is effective for many scenarios, some applications may not benefit from it, especially those with high inter-process communication requirements.


      When to Use Horizontal Scaling
         - Horizontal scaling is suitable for applications with dynamic or unpredictable workloads.
         - It's effective for cloud-based architectures where resources can be provisioned and deprovisioned on demand.
         - When redundancy, fault tolerance, and improved scalability are critical.

      Scalability principales
         - Decentralization - One component is not responsible for all the work. If one component is responsible for all the work it is called Monolithic. Monolithic is an anti-pattern for scalability. Decentralization in scalability refers to the distribution of responsibilities and functions across multiple components or nodes within a system rather than centralizing them in a single entity. In a decentralized architecture, different parts of the system can operate independently, reducing bottlenecks and improving overall scalability.
                Key Aspects of Decentralization
                     - Distribution of Components: Instead of having a monolithic architecture, decentralization involves breaking down the system into smaller, independent components or services. Each component can operate autonomously.
                     - Data Distribution: In a decentralized system, data is often distributed across multiple nodes. This can involve partitioning data or replicating it across various servers to ensure that no single point becomes a performance bottleneck.
                     - Load Distribution: Workloads are distributed across multiple nodes or instances, ensuring that no single component bears the entire burden of incoming requests. Load balancing mechanisms play a crucial role in achieving this distribution.
               
               Advantages of Decentralization
                   - Improved Scalability: Decentralization allows a system to scale horizontally by adding more nodes to distribute the workload, providing a more efficient way to handle increased demand.
                   - Fault Tolerance: Since responsibilities are distributed, failures in one component do not necessarily affect the entire system. Decentralized systems are often more resilient to faults.
                   - Flexibility: Decentralized architectures are more flexible and adaptable to changes in workload. Adding or removing nodes can be done without disrupting the entire system.

         - Independence - Independence in scalability refers to the ability of components or modules within a system to operate autonomously without strong dependencies on each other. Each component can function independently, making it easier to develop, deploy, and scale.
               Key Aspects of Independence
                    - Loose Coupling: Components in an independent system are loosely coupled, meaning that changes to one component do not have a significant impact on others. This allows for more flexibility and ease of modification.
                    - Isolation of Concerns: Each component or service focuses on a specific task or functionality, and its internal workings are encapsulated. This isolation of concerns makes it easier to reason about and maintain individual components.
                    - Service-Oriented Architecture (SOA): Independence is often achieved through a service-oriented architecture, where different functions are provided by independent services that communicate through well-defined interfaces.

               Advantages of Independence
                    - Easier Maintenance: Independent components are easier to maintain and update since changes to one component are less likely to affect others.
                    - Scalability: Independent components can scale horizontally without causing cascading effects on the rest of the system. This supports the overall scalability of the system.
                    - Parallel Development: Teams can work on different components concurrently, leading to faster development cycles and efficient collaboration.

     Load Balancer:
           -A load balancer is a device or software application that distributes incoming network traffic across multiple servers or resources to ensure optimal utilization, prevent overload on any single server, and enhance the availability and reliability of a system or application. Load balancers are commonly used to manage traffic for web servers, databases, and other types of server farms.
      - Key Functions of Load Balancers
             - Single IP Address: Client don't need to know about IP addresses of all instances. Load balancer will keep track of it and expose a single IP address for client.
             - Traffic Distribution: Load balancers evenly distribute incoming network traffic, such as web requests, across multiple servers. This ensures that no single server bears the entire load and prevents any server from becoming a performance bottleneck.
             - Load Distribution: By distributing the workload across multiple servers, load balancers optimize resource utilization, preventing overloading on specific servers. This leads to improved performance and responsiveness of the overall system.
             - Scalability: Load balancers support horizontal scalability by easily integrating additional servers into the system. This enables the infrastructure to grow or shrink dynamically based on demand, enhancing the system's overall scalability.
             - Health Monitoring: Load balancers continually monitor the health and performance of individual servers. If a server becomes unavailable or experiences degraded performance, the load balancer can redirect traffic to healthier servers to maintain system stability.
             - Session Persistence: Some load balancers support session persistence, ensuring that requests from the same client are consistently directed to the same server. This is essential for applications that require maintaining session state, such as in e-commerce websites.
      
      - Use Cases of Load Balancers
            - Web Servers and Applications: Load balancers distribute incoming web requests across multiple servers, ensuring even utilization and preventing any single server from becoming a bottleneck. This is common in websites, online applications, and e-commerce platforms.
            - Application Servers in Multi-Tier Architectures: In multi-tier architectures, load balancers distribute traffic among application servers, helping to balance the load and improve the overall performance of the application
            - Database Servers: Load balancers can be used to distribute read queries among multiple database servers, optimizing the use of database resources and improving query response times.
            - Content Delivery Networks (CDN): CDNS use load balancers to distribute content to edge servers located in different geographical locations. This reduces latency and improves the speed of content delivery for users around the world.
            - File Servers and Storage Clusters: Load balancers distribute file requests across multiple file servers or storage clusters, preventing any single server from becoming overwhelmed and ensuring efficient data retrieval.
            - Mail Servers (SMTP, IMAP): Load balancers distribute email traffic across multiple mail servers, ensuring efficient handling of incoming and outgoing emails and preventing any single server from being overwhelmed.

      - HLB vs SLB
          - Hardware Based Load Balancer
               • Load distribution for L4 & L7
               • OSI Model
               • F5 Big IP 15000 series
                     - connection: 300 million
                     - Throughput: 320/160 Gbps
                     - RPS (L7): 10 million

         - Software Based Load Balancer
               • Load distribution L7
               • Features
                    - Content based routing
                    - Support SSL Termination
                    - Support Sticky Sessions
               • NGINX:
                    - Connections: 225K
                    - Throughput: 70 Gbps
                    - RPS: 3 million
      - Reverse Proxy:
            - A reverse proxy is a server that sits between client devices and a web server, forwarding client requests to the web server and returning the server's responses to clients. Unlike a forward proxy that sits between client devices and the internet, a reverse proxy is positioned on the server side to handle requests on behalf of the server. Reverse proxies provide several benefits, including improved security, load distribution, and caching.
         - Use Cases of Reverse Proxy
              - Request Forwarding: Reverse proxies forward client requests to backend servers, acting as an intermediary that relays requests on behalf of the clients.
              - Load Distribution: Similar to load balancers, reverse proxies distribute incoming requests across multiple backend servers to optimize resource utilization and improve the overall performance of the system. •
              - SSL Termination: Reverse proxies can handle SSL/TLS encryption and decryption, relieving backend servers of the resource-intensive task of managing secure connections. This enhances server performance and simplifies SSL certificate management.
              - Caching: Reverse proxies can cache static content, such as images, CSS files, and other assets, to reduce the load on backend servers and improve response times for frequently requested content.
              - Compression: Reverse proxies can compress content before sending it to clients, reducing bandwidth usage and improving the overall speed of content delivery.
              - Security: Acting as a barrier between clients and backend servers, reverse proxies enhance security by concealing the internal server structure, preventing direct access to backend servers, and mitigating certain types of attacks.
              - Web Acceleration: Reverse proxies can accelerate web applications by serving as a middle layer between clients and servers, optimizing content delivery, and reducing latency.
              - SSL Offloading: In addition to SSL termination, reverse proxies can offload SSL processing tasks from backend servers, freeing up server resources for handling application logic.

      - Load Balancer vs Reverse Proxy

         Feature              -            Load Balancer                 -                                                   Reverse Proxy
          Location                           Between clients and multiple servers.                                             On the server side, in front of backend servers.
          Functionality                      Primarily focuses on load distribution.                                           Manages communication between clients and servers, handles SSL termination, caching, and security.
          Use Cases                          Distributing traffic for performance and availability.                            Caching, SSL termination, security, and serving as a barrier between clients and servers.
          SSL/TLS Handling                   Can handle SSL/TLS offloading for multiple servers.                               Performs SSL/TLS termination, offloading encryption/decryption from backend servers.
          Caching                            Primarily focused on load distribution, may have limited caching capabilities.    Often incorporates caching to store and serve static content, reducing the load on backend servers.
          Client Communication               Directs client requests to appropriate backend servers.                           Communicates with clients on behalf of backend servers, protecting servers from direct exposure to the internet
          Type                               Software and Hardware                                                             Software

      - API Gateway
          - An API Gateway is a server that acts as an intermediary between an application and a set of microservices or APIs (Application Programming Interfaces). It serves as a single entry point for multiple APIs, handling tasks such as request routing, composition, security, and protocol translation. API Gateways are a crucial component in modern software architectures, providing a centralized point for managing and securing API interactions.
          Features of API Gateway
            - Request Routing: API Gateways route incoming requests from clients to the appropriate backend services or APIs based on predefined rules and configurations.
            - Authentication and Authorization: Ensures that only authorized users or applications can access the APIs by implementing authentication mechanisms such as API keys, OAuth, or other authentication protocols.
            - Request and Response Transformation: Modifies or transforms incoming requests and outgoing responses to ensure compatibility between client and server, such as converting data formats or handling versioning.
            - Rate Limiting and Throttling: Implements controls to limit the number of requests a client can make within a specified time frame, preventing abuse and ensuring fair usage of API resources.
            - Logging and Analytics: Captures detailed logs of API requests and responses, providing insights into API usage, performance, and potential issues. Analytics tools help in monitoring and optimizing API performance.
            - Caching: Stores and serves cached responses for frequently requested data, reducing the load on backend servers and improving response times for clients.
            - Security: Enforces security measures such as HTTPS, SSL/TLS termination, and protection against common security threats like SQL injection or cross-site scripting.
            - Monitoring and Health Checks: Monitors the health and availability of backend services, performing health checks and dynamically adjusting routing based on the status of the services.
            - API Versioning: Supports versioning of APIs, allowing clients to specify the desired version and ensuring backward compatibility as APIs evolve.
            - Distributed Tracing: Enables the tracking of requests across multiple microservices, providing insights into the flow of requests and helping identify performance bottlenecks.

      - Use Cases of API Gateway
          - API Management: Centralizes the management of APIs, making it easier to create, deploy, version, and retire APIs.
          - Security and Access Control: Enforces security policies, authenticates users, and ensures that only authorized clients can access specific APIs.
          - Request Transformation and Composition: Modifies or combines API requests to suit backend service expectations, optimizing the communication between clients and microservices.
          - Distributed Microservices Architecture: Serves as the entry point for client interactions with microservices, handling the complexity of multiple services and ensuring a unified API surface.
          - Legacy System Integration: Bridges the gap between modern applications and legacy systems, allowing newer applications to interact with older services through a standardized API.
          - Cross-Origin Resource Sharing (CORS): Facilitates secure cross-origin communication by enforcing CORS policies, allowing web applications to securely make requests to APIs hosted on different domains.
          - API Versioning and Evolution: Manages API versioning, ensuring backward compatibility and smooth transitions as APIs evolve over time.
          - Ingress Controller for Kubernetes: An API Gateway can serve as an Ingress Controller in Kubernetes, managing external access to services, handling SSL termination, and performing request routing based on domain
          - Third-Party Integration: Facilitates integration with third-party APIs and services, managing API keys, authentication, and data translation to ensure seamless communication between different services.
          - Microservices Communication: Serves as a central point for communication between microservices within an architecture, managing the complexities of service discovery, load balancing, and fault tolerance.

      - Popular API Gateways
           - Kong
           - Apache APISIX
           - Tyk
           - Ocelot
           - Amazon API Gateway
           - Azure

      - Replication
          - Replication in system design refers to the process of creating and maintaining multiple copies of data, components, or systems to improve reliability, fault tolerance, and performance. The purpose of replication is to ensure that a system remains available and functional even in the face of failures, outages, or increased demand.

          - Key Concept
             - Data Replication
                 - Copies of data are maintained across multiple locations, servers, or storage devices.
                 - This approach improves data availability, reduces latency, and enhances fault tolerance.
            - Component Replication
                 - Replicating entire components or services involves creating multiple instances of a service or application that can operate independently
            
            - Stateless Replication
               - Stateless replication involves replicating the functionality of a system or service without necessarily replicating its state. Each replica operates independently, and there is no shared state between replicas.
               - Key Characteristics:
                  - No Shared State: Stateless replicas do not share a common state. Each replica is responsible for managing its own state independently.
                  - Scalability: Stateless replication is often more scalable than stateful replication because each replica can operate independently, and there is no need for constant synchronization.
                  - Simplicity: Stateless replicas are typically simpler to implement, as there is no need to manage shared state or handle complex synchronization mechanisms.
                  - Use Cases: Stateless replication is common in scenarios where maintaining an identical state across replicas is not necessary, such as load balancing in web servers, content delivery networks (CDNs), and stateless microservices.
            - Stateful Replication
               - Stateful replication involves replicating both the data and the state of a system. In stateful replication, each instance or replica maintains the current state, and changes made to one replica are reflected in others.
               - Key Characteristics:
                  - Shared State: Stateful replicas share the same state, ensuring that each replica has an up-to-date copy of the system's data and state.
                  - Consistency: Maintaining consistency across replicas is critical in stateful replication. Changes made to one replica are propagated to others to ensure a coherent state.
                  - Challenges: Achieving stateful replication can be challenging, especially in distributed systems, as it requires mechanisms to synchronize and coordinate the state across replicas.
                  - Use Cases: Stateful replication is often used in scenarios where maintaining a consistent state across replicas is essential, such as in databases, distributed file systems, and certain types of distributed applications.
               - Stateful Replication - Web Application
                  - Not Recommended
                  - Low latency but lack of scalability and reliability
                  - When low latency is required such as Auth Session
                  - Manage sessions using
                        - Sticky session / session affinity
                        - Session clustering
               - Stateful Replication - Database
                     - Master Slave (Primary Secondary)
                        - Asynchronous
                            - Low latency writes
                            - Eventually consistent
                            - Data Loss
                            - Used for Read Replica
                        - Synchronous
                           - Consistent
                           - High latency writes
                           - Low write availability
                           - Used for Backup
                     - Master Slave (Peer to peer)
                          - Asynchronous
                              - Write conflicts
                              - High availability
                              - Use for multi regional service






•
•
•
•
•
•